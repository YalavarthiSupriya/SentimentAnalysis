{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "df = pd.read_csv('IMDB.csv')\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2))\n",
    "text_count_matrix = tfidf.fit_transform(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the complete dataset in test and training dataset:\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_count_matrix, df.sentiment, test_size=0.30, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the sentiments (positive and negatives) to 1 and 0. \n",
    "y_train = (y_train.replace({'positive': 1, 'negative': 0})).values\n",
    "y_test = (y_test.replace({'positive': 1, 'negative': 0})).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score without data pre-processing = 89.17 %\n"
     ]
    }
   ],
   "source": [
    "# let's use Naive Bayes classifier and fit our model:\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(x_train, y_train)\n",
    "#4. Evaluating the model\n",
    "from sklearn import metrics\n",
    "accuracy_score = metrics.accuracy_score(MNB.predict(x_test), y_test)\n",
    "print(\"accuracy_score without data pre-processing = \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\n",
    "\n",
    " #lemmatized_sentence = \" \".join(lemmatizer.lemmatize(token) for token in word_tokens if token not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phil Alien one quirky films humour based around oddness everything rather actual punchlines At first odd pretty funny movie progressed find jokes oddness funny anymore Its low budget film thats never problem pretty interesting characters eventually lost interest imagine film would appeal stoner currently partaking For something similar better try Brother another planet\n"
     ]
    }
   ],
   "source": [
    "#let's investigate what kind of special characters and language is used by the reviewers to review the content. \n",
    "#we can observe some html tags\n",
    "#use of parenthesis\n",
    "#punctuation (apostrophy, '' e.t.c)\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "processed_review = []\n",
    "single_review = \"string to iniialize <br /> my email id is charilie@waoow.com. You can also reach to me at charlie's \"\n",
    "reviews = df.review\n",
    "for review in range(0,50000):\n",
    "    single_review = df.loc[review,'review']\n",
    "    #start processing the single_review #removing html tags:\n",
    "    single_review = re.sub('<.*?>',' ',single_review)\n",
    "    #removing special characters (punctuation) '@,!' e.t.c.\n",
    "    single_review = re.sub('\\W',' ',single_review)\n",
    "    #removing single characters\n",
    "    single_review = re.sub('\\s+[a-zA-Z]\\s+',' ', single_review)\n",
    "    #substituting multiple spaces with single space\n",
    "    single_review = re.sub('\\s+',' ', single_review)\n",
    "    #removing stop words#word_tokens = []\n",
    "    word_tokens = word_tokenize(single_review)\n",
    "    #lemmatization\n",
    "    filtered_sentence = []\n",
    "    #filtered_sentence.append([w for w in word_tokens if w not in stop_words])\n",
    "    filtered_sentence2 = \" \".join([w for w in word_tokens if w not in stop_words])\n",
    "    #compile all the sentences to make a complete dictionary of processed reviews\n",
    "    processed_review.append(filtered_sentence2)\n",
    "print(processed_review[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after preprocessing using Naivebayes 89.28 %\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.89      0.89      7499\n",
      "    Positive       0.89      0.89      0.89      7501\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6689  810]\n",
      " [ 798 6703]]\n"
     ]
    }
   ],
   "source": [
    "text_count_matrix2 = tfidf.fit_transform(processed_review)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_count_matrix2, df.sentiment, test_size=0.30, random_state=2)\n",
    "Y_train = (Y_train.replace({'positive': 1, 'negative': 0})).values\n",
    "Y_test = (Y_test.replace({'positive': 1, 'negative': 0})).values\n",
    "MNB.fit(X_train, Y_train)\n",
    "#4. Evaluating the model\n",
    "accuracy_score = metrics.accuracy_score(MNB.predict(X_test), Y_test)\n",
    "print('Accuracy after preprocessing using Naivebayes',str('{:04.2f}'.format(accuracy_score*100))+\" %\")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(Y_test, MNB.predict(X_test),target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, MNB.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocastic Gradient Classifier accuracy = 90.08 %\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.88      0.90      7499\n",
      "    Positive       0.89      0.92      0.90      7501\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6609  890]\n",
      " [ 598 6903]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "SGDC = SGDClassifier()\n",
    "SGDC.fit(X_train, Y_train)\n",
    "predict = SGDC.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predict, Y_test)\n",
    "print(\"Stocastic Gradient Classifier accuracy = \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\n",
    "print(\"Classification Report: \\n\", classification_report(Y_test, predict,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC accuracy = 91.13 %\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.90      0.91      7499\n",
      "    Positive       0.90      0.92      0.91      7501\n",
      "\n",
      "    accuracy                           0.91     15000\n",
      "   macro avg       0.91      0.91      0.91     15000\n",
      "weighted avg       0.91      0.91      0.91     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6746  753]\n",
      " [ 578 6923]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "LSVC = LinearSVC()\n",
    "LSVC.fit(X_train, Y_train)\n",
    "accuracy_score = metrics.accuracy_score(LSVC.predict(X_test), Y_test)\n",
    "print(\"Linear SVC accuracy = \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\n",
    "print(\"Classification Report: \\n\", classification_report(Y_test, LSVC.predict(X_test),target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, LSVC.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy = 89.78 %\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.88      0.90      7499\n",
      "    Positive       0.89      0.91      0.90      7501\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6611  888]\n",
      " [ 645 6856]]\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, Y_train)\n",
    "predict = LR.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predict, Y_test)\n",
    "print(\"Logistic Regression Accuracy = \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\n",
    "print(\"Classification Report: \\n\", classification_report(Y_test, predict,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Accuracy = 67.89 %\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.48      0.60      7499\n",
      "    Positive       0.63      0.88      0.73      7501\n",
      "\n",
      "    accuracy                           0.68     15000\n",
      "   macro avg       0.71      0.68      0.67     15000\n",
      "weighted avg       0.71      0.68      0.67     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[3597 3902]\n",
      " [ 914 6587]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\",random_state = 100,max_depth=3, min_samples_leaf=5)\n",
    "clf_gini.fit(X_train, y_train)\n",
    "predict = clf_gini.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predict, Y_test)\n",
    "print(\"Decision Tree Classifier Accuracy = \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\n",
    "print(\"Classification Report: \\n\", classification_report(Y_test, predict,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy=90.55 %\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.90      0.90      7499\n",
      "    Positive       0.90      0.91      0.91      7501\n",
      "\n",
      "    accuracy                           0.91     15000\n",
      "   macro avg       0.91      0.91      0.91     15000\n",
      "weighted avg       0.91      0.91      0.91     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6751  748]\n",
      " [ 670 6831]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "models = [('lr',LogisticRegression()),('MNB',MultinomialNB()),('SGDC',SGDClassifier()),('LSVC',LinearSVC())]\n",
    "ensemble = VotingClassifier(estimators=models)\n",
    "ensemble.fit(X_train, Y_train)\n",
    "predict =ensemble.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predict, Y_test)\n",
    "print(\"Voting Classifier Accuracy=\" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\n",
    "print(\"Classification Report: \\n\", classification_report(Y_test, predict,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy = 90.55 %\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.90      0.90      7499\n",
      "    Positive       0.90      0.91      0.91      7501\n",
      "\n",
      "    accuracy                           0.91     15000\n",
      "   macro avg       0.91      0.91      0.91     15000\n",
      "weighted avg       0.91      0.91      0.91     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6751  748]\n",
      " [ 670 6831]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "classifier= RandomForestClassifier(n_estimators= 5, criterion=\"entropy\")  \n",
    "classifier.fit(X_train, Y_train)  \n",
    "Y_pred= classifier.predict(X_test)  \n",
    "accuracy_score = metrics.accuracy_score(predict, Y_test)\n",
    "print(\"Random Forest Classifier Accuracy = \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\n",
    "print(\"Classification Report: \\n\", classification_report(Y_test, predict,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Regressor Accuracy 90.55 %\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.90      0.90      7499\n",
      "    Positive       0.90      0.91      0.91      7501\n",
      "\n",
      "    accuracy                           0.91     15000\n",
      "   macro avg       0.91      0.91      0.91     15000\n",
      "weighted avg       0.91      0.91      0.91     15000\n",
      "\n",
      "Confusion Matrix: \n",
      " [[6751  748]\n",
      " [ 670 6831]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred= model.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predict, Y_test)\n",
    "print(\"Gradient Boost Regressor Accuracy \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\n",
    "print(\"Classification Report: \\n\", classification_report(Y_test, predict,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
